# Исследование эффективности оптимизаторов для MLP

## Описание проекта

Этот проект посвящен исследованию влияния глубины нейронной сети на эффективность различных оптимизаторов: Adam, RMSProp и Lion. Цель работы — определить, насколько каждый из оптимизаторов сохраняет свою эффективность при увеличении глубины сети, а также выявить оптимальные сценарии их использования. В исследовании используются три датасета: Wine Quality, Concrete Strength и Energy Efficiency. Результаты включают анализ скорости сходимости, стабильности обучения и итогового качества моделей.

## Ключевые особенности

- Сравнение оптимизаторов Adam, RMSProp и Lion на MLP с разной глубиной (2, 4, 8 слоев).
- Использование двух архитектур: базовая модель и модель с регуляризацией (Dropout).
- Оценка качества моделей с помощью метрик MSE и MAE.

## Датасеты, на которых проводились эксперименты

1. **Wine Quality**: Предсказание оценки вина на основе физико-химических свойств.
2. **Concrete Strength**: Предсказание прочности бетона на сжатие.
3. **Energy Efficiency**: Предсказание энергопотребления для отопления зданий.

## Структура проекта
```
summer_school_2025/
├── .github/
│ └── workflows/
│ └── c.yml
├── notebooks/
│ └── summer_school_notebook.ipynb # Jupyter notebook с материалами
├── tests/
│ ├── src/
│ │    └──for_test.py # Классы для тестов
│ └── test_notebook.py # Тесты для notebook
├── .gitignore
├── .pre-commit-config.yaml
├── LICENSE
├── README.md
├── requirements.txt
└── setup.py
```
